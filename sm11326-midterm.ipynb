{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9465ff89-9e2b-4fc5-be0b-8879532c847a",
   "metadata": {},
   "source": [
    "# Spark initialization - spark template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d03af9-d433-4d41-a766-bcbda55253ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/08 00:17:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-sm11326:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code><my-app-name></code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=<my-app-name>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf = conf.setAppName(\"<my-app-name>\")\n",
    "conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4040') ## to setup SPARK UI\n",
    "conf = conf.set('spark.jars', os.environ['GRAPHFRAMES_PATH']) ## graphframes in spark configuration\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7e1af-a5ba-47a6-bdbc-c3d0e4b8e542",
   "metadata": {},
   "source": [
    "### Open Spark UI\n",
    "``` https://csgy1-6513-fall.rcnyu.org/user/<USER_NETID>/proxy/4040/jobs/  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c52cd742-faff-4097-a6e6-1a2003f036b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/08 00:18:00 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------+\n",
      "|trigram          |trigram_count|conditional_probability|\n",
      "+-----------------+-------------+-----------------------+\n",
      "|[@, @, @]        |113349       |0.8886563021850084     |\n",
      "|[., <p>, \"]      |7959         |0.16716022934912733    |\n",
      "|[., <p>, The]    |6287         |0.13204376955873395    |\n",
      "|[., \", <p>]      |4810         |0.4664468580294802     |\n",
      "|[said, ., <p>]   |2415         |0.5625436757512229     |\n",
      "|[,, \", he]       |1487         |0.16955530216647663    |\n",
      "|[<p>, \", We]     |1376         |0.15076147693656186    |\n",
      "|[., <p>, In]     |1321         |0.02774452355449142    |\n",
      "|[the, spread, of]|1180         |0.9161490683229814     |\n",
      "|[,, \", said]     |1142         |0.13021664766248575    |\n",
      "+-----------------+-------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 2 - WITHOUT ANY FILTERING - I.E INCLUSIVE OF ALL PUNCTUATIONS [ @, ., , , < , >, p ]\n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, count, concat_ws, expr\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TrigramLanguageModel\").getOrCreate()\n",
    "\n",
    "input_path = \"*.txt\"\n",
    "text_df = spark.read.text(input_path)\n",
    "\n",
    "words_df = text_df.select(split(col(\"value\"), \"\\s+\").alias(\"words\"))\n",
    "\n",
    "words_df = words_df.filter(\"size(words) >= 3\")\n",
    "\n",
    "bigrams_df = words_df.select(\n",
    "    explode(expr(\"transform(sequence(0, size(words) - 2), i -> array(words[i], words[i + 1]))\")).alias(\"bigram\")\n",
    ")\n",
    "trigrams_df = words_df.select(\n",
    "    explode(expr(\"transform(sequence(0, size(words) - 3), i -> array(words[i], words[i + 1], words[i + 2]))\")).alias(\"trigram\")\n",
    ")\n",
    "\n",
    "bigram_counts = bigrams_df.groupBy(\"bigram\").count().withColumnRenamed(\"count\", \"bigram_count\")\n",
    "trigram_counts = trigrams_df.groupBy(\"trigram\").count().withColumnRenamed(\"count\", \"trigram_count\")\n",
    "trigram_counts = trigram_counts.withColumn(\"bigram_prefix\", concat_ws(\" \", col(\"trigram\")[0], col(\"trigram\")[1]))\n",
    "bigram_counts = bigram_counts.withColumn(\"bigram_str\", concat_ws(\" \", col(\"bigram\")[0], col(\"bigram\")[1]))\n",
    "conditional_df = trigram_counts.join(\n",
    "    bigram_counts,\n",
    "    trigram_counts.bigram_prefix == bigram_counts.bigram_str\n",
    ").select(\n",
    "    \"trigram\", \"trigram_count\", \"bigram_count\"\n",
    ").withColumn(\n",
    "    \"conditional_probability\", col(\"trigram_count\") / col(\"bigram_count\")\n",
    ")\n",
    "\n",
    "\n",
    "top_trigrams = conditional_df.orderBy(col(\"trigram_count\").desc()).limit(10)\n",
    "top_trigrams.select(\"trigram\", \"trigram_count\", \"conditional_probability\").show(truncate=False)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42677ac4-651b-42e0-8449-2c47226ad50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------+-----------------------+\n",
      "|trigram                     |trigram_count|conditional_probability|\n",
      "+----------------------------+-------------+-----------------------+\n",
      "|[the, spread, of]           |1180         |0.9161490683229814     |\n",
      "|[of, the, coronavirus]      |854          |0.04978430686720298    |\n",
      "|[as, well, as]              |824          |0.7285587975243147     |\n",
      "|[the, number, of]           |819          |0.9457274826789839     |\n",
      "|[one, of, the]              |791          |0.6233254531126872     |\n",
      "|[spread, of, the]           |772          |0.5482954545454546     |\n",
      "|[due, to, the]              |719          |0.42244418331374856    |\n",
      "|[the, coronavirus, pandemic]|711          |0.20887191539365452    |\n",
      "|[of, the, virus]            |691          |0.04028214993587501    |\n",
      "|[the, end, of]              |615          |0.9057437407952872     |\n",
      "+----------------------------+-------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUESTION 2 - STRICT VERSION - WHERE I HAVE REGEX FILTERED EVERYTHING APART FROM THE ALPHA-NUMERIC\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, count, concat_ws, expr\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TrigramLanguageModel\").getOrCreate()\n",
    "input_path = \"*.txt\"\n",
    "text_df = spark.read.text(input_path)\n",
    "words_df = text_df.select(split(col(\"value\"), \"\\s+\").alias(\"words\"))\n",
    "words_df = words_df.filter(\"size(words) >= 3\")\n",
    "bigrams_df = words_df.select(\n",
    "    explode(expr(\"transform(sequence(0, size(words) - 2), i -> array(words[i], words[i + 1]))\")).alias(\"bigram\")\n",
    ").filter(\n",
    "    (col(\"bigram\")[0].rlike(\"^[A-Za-z0-9]+$\")) & (col(\"bigram\")[1].rlike(\"^[A-Za-z0-9]+$\"))\n",
    ")\n",
    "\n",
    "trigrams_df = words_df.select(\n",
    "    explode(expr(\"transform(sequence(0, size(words) - 3), i -> array(words[i], words[i + 1], words[i + 2]))\")).alias(\"trigram\")\n",
    ").filter(\n",
    "    (col(\"trigram\")[0].rlike(\"^[A-Za-z0-9]+$\")) & \n",
    "    (col(\"trigram\")[1].rlike(\"^[A-Za-z0-9]+$\")) & \n",
    "    (col(\"trigram\")[2].rlike(\"^[A-Za-z0-9]+$\"))\n",
    ")\n",
    "\n",
    "bigram_counts = bigrams_df.groupBy(\"bigram\").count().withColumnRenamed(\"count\", \"bigram_count\")\n",
    "trigram_counts = trigrams_df.groupBy(\"trigram\").count().withColumnRenamed(\"count\", \"trigram_count\")\n",
    "trigram_counts = trigram_counts.withColumn(\"bigram_prefix\", concat_ws(\" \", col(\"trigram\")[0], col(\"trigram\")[1]))\n",
    "bigram_counts = bigram_counts.withColumn(\"bigram_str\", concat_ws(\" \", col(\"bigram\")[0], col(\"bigram\")[1]))\n",
    "\n",
    "conditional_df = trigram_counts.join(\n",
    "    bigram_counts,\n",
    "    trigram_counts.bigram_prefix == bigram_counts.bigram_str\n",
    ").select(\n",
    "    \"trigram\", \"trigram_count\", \"bigram_count\"\n",
    ").withColumn(\n",
    "    \"conditional_probability\", col(\"trigram_count\") / col(\"bigram_count\")\n",
    ")\n",
    "\n",
    "top_trigrams = conditional_df.orderBy(col(\"trigram_count\").desc()).limit(10)\n",
    "top_trigrams.select(\"trigram\", \"trigram_count\", \"conditional_probability\").show(truncate=False)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f93afd0-99b6-4ed0-92ac-3940b74f10f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------+-----------------------+\n",
      "|trigram                     |trigram_count|conditional_probability|\n",
      "+----------------------------+-------------+-----------------------+\n",
      "|[the, spread, of]           |1180         |0.9161490683229814     |\n",
      "|[of, the, coronavirus]      |854          |0.04978430686720298    |\n",
      "|[as, well, as]              |824          |0.7285587975243147     |\n",
      "|[the, number, of]           |819          |0.9457274826789839     |\n",
      "|[one, of, the]              |791          |0.6233254531126872     |\n",
      "|[spread, of, the]           |772          |0.5482954545454546     |\n",
      "|[due, to, the]              |719          |0.42244418331374856    |\n",
      "|[the, coronavirus, pandemic]|711          |0.20887191539365452    |\n",
      "|[of, the, virus]            |691          |0.04028214993587501    |\n",
      "|[the, end, of]              |615          |0.9057437407952872     |\n",
      "+----------------------------+-------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUESTION 2 : BROADCASTING VERSION \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, count, concat_ws, expr, broadcast\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TrigramLanguageModel\").getOrCreate()\n",
    "input_path = \"*.txt\"\n",
    "text_df = spark.read.text(input_path)\n",
    "words_df = text_df.select(split(col(\"value\"), \"\\s+\").alias(\"words\"))\n",
    "words_df = words_df.filter(\"size(words) >= 3\")\n",
    "\n",
    "bigrams_df = words_df.select(\n",
    "    explode(expr(\"transform(sequence(0, size(words) - 2), i -> array(words[i], words[i + 1]))\")).alias(\"bigram\")\n",
    ").filter(\n",
    "    (col(\"bigram\")[0].rlike(\"^[A-Za-z0-9]+$\")) & (col(\"bigram\")[1].rlike(\"^[A-Za-z0-9]+$\"))\n",
    ")\n",
    "\n",
    "trigrams_df = words_df.select(\n",
    "    explode(expr(\"transform(sequence(0, size(words) - 3), i -> array(words[i], words[i + 1], words[i + 2]))\")).alias(\"trigram\")\n",
    ").filter(\n",
    "    (col(\"trigram\")[0].rlike(\"^[A-Za-z0-9]+$\")) & \n",
    "    (col(\"trigram\")[1].rlike(\"^[A-Za-z0-9]+$\")) & \n",
    "    (col(\"trigram\")[2].rlike(\"^[A-Za-z0-9]+$\"))\n",
    ")\n",
    "\n",
    "bigram_counts = bigrams_df.groupBy(\"bigram\").count().withColumnRenamed(\"count\", \"bigram_count\")\n",
    "trigram_counts = trigrams_df.groupBy(\"trigram\").count().withColumnRenamed(\"count\", \"trigram_count\")\n",
    "trigram_counts = trigram_counts.withColumn(\"bigram_prefix\", concat_ws(\" \", col(\"trigram\")[0], col(\"trigram\")[1]))\n",
    "bigram_counts = bigram_counts.withColumn(\"bigram_str\", concat_ws(\" \", col(\"bigram\")[0], col(\"bigram\")[1]))\n",
    "\n",
    "conditional_df = trigram_counts.join(\n",
    "    broadcast(bigram_counts),\n",
    "    trigram_counts.bigram_prefix == bigram_counts.bigram_str\n",
    ").select(\n",
    "    \"trigram\", \"trigram_count\", \"bigram_count\"\n",
    ").withColumn(\n",
    "    \"conditional_probability\", col(\"trigram_count\") / col(\"bigram_count\")\n",
    ")\n",
    "\n",
    "top_trigrams = conditional_df.orderBy(col(\"trigram_count\").desc()).limit(10)\n",
    "top_trigrams.select(\"trigram\", \"trigram_count\", \"conditional_probability\").show(truncate=False)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce52ea1-66ae-4dbf-98cc-2f36fa263955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+\n",
      "|Daypart  |TopItems               |\n",
      "+---------+-----------------------+\n",
      "|afternoon|[Coffee, Bread, Tea]   |\n",
      "|evening  |[Coffee, Bread, Tea]   |\n",
      "|morning  |[Coffee, Bread, Pastry]|\n",
      "|noon     |[Coffee, Bread, Tea]   |\n",
      "+---------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### QUESTION 3\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, hour, when, count, rank, collect_list\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Top 3 Items Per Daypart\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"shared/data/Bakery.csv\")\n",
    "df = df.withColumn(\"Time\", col(\"Time\").cast(\"timestamp\"))\n",
    "df = df.withColumn(\"Daypart\", \n",
    "                   when((hour(col(\"Time\")) >= 6) & (hour(col(\"Time\")) < 11), \"morning\")\n",
    "                   .when((hour(col(\"Time\")) >= 11) & (hour(col(\"Time\")) < 14), \"noon\")\n",
    "                   .when((hour(col(\"Time\")) >= 14) & (hour(col(\"Time\")) < 17), \"afternoon\")\n",
    "                   .when((hour(col(\"Time\")) >= 17) | (hour(col(\"Time\")) < 6), \"evening\")\n",
    "                   )\n",
    "\n",
    "item_counts = df.groupBy(\"Daypart\", \"Item\").agg(count(\"Item\").alias(\"count\"))\n",
    "window_spec = Window.partitionBy(\"Daypart\").orderBy(col(\"count\").desc())\n",
    "\n",
    "ranked_items = item_counts.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 3)\n",
    "\n",
    "top_items_per_daypart = ranked_items \\\n",
    "    .groupBy(\"Daypart\") \\\n",
    "    .agg(collect_list(\"Item\").alias(\"TopItems\"))\n",
    "\n",
    "top_items_per_daypart.show(truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06e371fb-c49f-4463-83a6-967d77f4c2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw item counts per daypart:\n",
      "+---------+--------------------------+-----+\n",
      "|Daypart  |Item                      |count|\n",
      "+---------+--------------------------+-----+\n",
      "|noon     |Bare Popcorn              |1    |\n",
      "|noon     |My-5 Fruit Shoot          |7    |\n",
      "|morning  |Jammie Dodgers            |22   |\n",
      "|noon     |Christmas common          |5    |\n",
      "|evening  |Focaccia                  |3    |\n",
      "|morning  |Chocolates                |2    |\n",
      "|noon     |Drinking chocolate spoons |2    |\n",
      "|afternoon|Empanadas                 |3    |\n",
      "|afternoon|Cherry me Dried fruit     |1    |\n",
      "|afternoon|Cake                      |480  |\n",
      "|afternoon|Extra Salami or Feta      |15   |\n",
      "|afternoon|Scone                     |127  |\n",
      "|morning  |Muffin                    |79   |\n",
      "|morning  |NONE                      |201  |\n",
      "|evening  |Cookies                   |21   |\n",
      "|afternoon|Bowl Nic Pitt             |1    |\n",
      "|evening  |Juice                     |13   |\n",
      "|morning  |Truffles                  |16   |\n",
      "|morning  |Empanadas                 |1    |\n",
      "|noon     |Tacos/Fajita              |6    |\n",
      "+---------+--------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Top 3 Items per Daypart (Ranked):\n",
      "+---------+-----------------------+\n",
      "|Daypart  |TopItems               |\n",
      "+---------+-----------------------+\n",
      "|afternoon|[Coffee, Bread, Tea]   |\n",
      "|evening  |[Coffee, Bread, Tea]   |\n",
      "|morning  |[Coffee, Bread, Pastry]|\n",
      "|noon     |[Coffee, Bread, Tea]   |\n",
      "+---------+-----------------------+\n",
      "\n",
      "\n",
      "Verification: Raw counts for the top 3 items per daypart:\n",
      "+---------+------+-----+\n",
      "|Daypart  |Item  |count|\n",
      "+---------+------+-----+\n",
      "|afternoon|Coffee|1476 |\n",
      "|afternoon|Bread |847  |\n",
      "|afternoon|Tea   |566  |\n",
      "|evening  |Coffee|87   |\n",
      "|evening  |Bread |55   |\n",
      "|evening  |Tea   |49   |\n",
      "|morning  |Coffee|1615 |\n",
      "|morning  |Bread |1081 |\n",
      "|morning  |Pastry|453  |\n",
      "|noon     |Coffee|2293 |\n",
      "|noon     |Bread |1342 |\n",
      "|noon     |Tea   |540  |\n",
      "+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### QUESTION 3 - VERIFICATION\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, hour, when, count, rank, collect_list, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Top 3 Items Per Daypart with Verification\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"shared/data/Bakery.csv\")\n",
    "df = df.withColumn(\"Time\", col(\"Time\").cast(\"timestamp\"))\n",
    "df = df.withColumn(\"Daypart\", \n",
    "                   when((hour(col(\"Time\")) >= 6) & (hour(col(\"Time\")) < 11), \"morning\")\n",
    "                   .when((hour(col(\"Time\")) >= 11) & (hour(col(\"Time\")) < 14), \"noon\")\n",
    "                   .when((hour(col(\"Time\")) >= 14) & (hour(col(\"Time\")) < 17), \"afternoon\")\n",
    "                   .when((hour(col(\"Time\")) >= 17) | (hour(col(\"Time\")) < 6), \"evening\")\n",
    "                   )\n",
    "\n",
    "item_counts = df.groupBy(\"Daypart\", \"Item\").agg(count(\"Item\").alias(\"count\"))\n",
    "print(\"Raw item counts per daypart:\")\n",
    "item_counts.show(truncate=False)\n",
    "window_spec = Window.partitionBy(\"Daypart\").orderBy(col(\"count\").desc())\n",
    "ranked_items = item_counts.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 3)\n",
    "\n",
    "top_items_per_daypart = ranked_items \\\n",
    "    .groupBy(\"Daypart\") \\\n",
    "    .agg(collect_list(\"Item\").alias(\"TopItems\"))\n",
    "\n",
    "print(\"\\nTop 3 Items per Daypart (Ranked):\")\n",
    "top_items_per_daypart.show(truncate=False)\n",
    "\n",
    "print(\"\\nVerification: Raw counts for the top 3 items per daypart:\")\n",
    "verification = ranked_items.select(\"Daypart\", \"Item\", \"count\").orderBy(\"Daypart\", \"rank\")\n",
    "verification.show(truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758274a8-e9d8-4912-a3d5-c7db56bab6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "|link                                                                                                                                          |headline                                                                      |category     |short_description                                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "|https://www.huffingtonpost.com/entry/mom-hilariously-nails-what-getting-ready-looks-like-for-mothers_us_595666f0e4b0da2c73230b40              |Mom Hilariously Nails What Getting Ready Looks Like For Mothers               |PARENTS      |”Maybe she’s born with it ... Maybe she’s a tired mom who doesn’t have time for this.”                                  |\n",
      "|https://www.huffingtonpost.com/entry/andrew-garfield-lip-syncs-whitney-houston-in-epic-drag-show-act_us_592ff5d1e4b0540ffc84b79b              |Andrew Garfield Lip-Syncs Whitney Houston In Epic Drag Show Act               |ENTERTAINMENT|With a back flip and everything.                                                                                        |\n",
      "|https://www.huffingtonpost.com/entry/linked-by-their-choice-to-become-single-mothers-two-women-share-their-stories_us_59b91b63e4b0edff9717de69|Linked By Their Choice To Become Single Mothers, Two Women Share Their Stories|PARENTS      |The stories of an Idaho mom who gave birth and a California mom who adopted.                                            |\n",
      "|https://www.huffingtonpost.com/entry/carol-brady_us_5b9c5c92e4b03a1dcc7e15be                                                                  |What Carol Brady Is Really Saying (INFOGRAPHIC)                               |PARENTING    |Here's the story of a lovely lady -– who somehow kept six kids, a husband, a housekeeper and a dog in perfect order with|\n",
      "|https://www.huffingtonpost.com/entry/britney-spears-new-album_us_563a0b2fe4b0307f2cab4995                                                     |Britney Spears' Comeback Is About To Be Complete                              |ENTERTAINMENT|She’s back, b***hes, with a brand new album.                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# QUESTION 4 : HASHINGTF, MINHASH-LSH - APPROX NEAREST NEIGHBORS - JACCARD SIMILARITY - 50 points\n",
    "# COMPUTING THE TOP 5 NEAREST URLS\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, MinHashLSH\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MinhashLSH\").getOrCreate()\n",
    "data_path = 'shared/data/Huffpost.json'\n",
    "df = spark.read.json(data_path)\n",
    "base_description = \"Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She’s Perfect\"\n",
    "tokenizer = Tokenizer(inputCol=\"short_description\", outputCol=\"words\")\n",
    "words_df = tokenizer.transform(df)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=10000)\n",
    "featurized_df = hashingTF.transform(words_df)\n",
    "minhash = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n",
    "model = minhash.fit(featurized_df)\n",
    "transformed_df = model.transform(featurized_df)\n",
    "base_df = spark.createDataFrame([(base_description,)], [\"short_description\"])\n",
    "base_words_df = tokenizer.transform(base_df)\n",
    "base_features_df = hashingTF.transform(base_words_df)\n",
    "similar_items = model.approxNearestNeighbors(transformed_df, base_features_df.select(\"features\").first()[\"features\"], numNearestNeighbors=5)\n",
    "similar_items.select(\"link\", \"headline\", \"category\", \"short_description\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5139386-46a8-4b4f-8d74-9507d1e05811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/08 00:20:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 items using MinHashLSH:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "|link                                                                                                                                          |headline                                                                      |category     |short_description                                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "|https://www.huffingtonpost.com/entry/mom-hilariously-nails-what-getting-ready-looks-like-for-mothers_us_595666f0e4b0da2c73230b40              |Mom Hilariously Nails What Getting Ready Looks Like For Mothers               |PARENTS      |”Maybe she’s born with it ... Maybe she’s a tired mom who doesn’t have time for this.”                                  |\n",
      "|https://www.huffingtonpost.com/entry/andrew-garfield-lip-syncs-whitney-houston-in-epic-drag-show-act_us_592ff5d1e4b0540ffc84b79b              |Andrew Garfield Lip-Syncs Whitney Houston In Epic Drag Show Act               |ENTERTAINMENT|With a back flip and everything.                                                                                        |\n",
      "|https://www.huffingtonpost.com/entry/linked-by-their-choice-to-become-single-mothers-two-women-share-their-stories_us_59b91b63e4b0edff9717de69|Linked By Their Choice To Become Single Mothers, Two Women Share Their Stories|PARENTS      |The stories of an Idaho mom who gave birth and a California mom who adopted.                                            |\n",
      "|https://www.huffingtonpost.com/entry/carol-brady_us_5b9c5c92e4b03a1dcc7e15be                                                                  |What Carol Brady Is Really Saying (INFOGRAPHIC)                               |PARENTING    |Here's the story of a lovely lady -– who somehow kept six kids, a husband, a housekeeper and a dog in perfect order with|\n",
      "|https://www.huffingtonpost.com/entry/britney-spears-new-album_us_563a0b2fe4b0307f2cab4995                                                     |Britney Spears' Comeback Is About To Be Complete                              |ENTERTAINMENT|She’s back, b***hes, with a brand new album.                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Top 5 items using Manual Jaccard Similarity:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|link                                                                                                                                          |headline                                                                      |category     |short_description                                                                                                       |jaccard_similarity |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|https://www.huffingtonpost.com/entry/mom-hilariously-nails-what-getting-ready-looks-like-for-mothers_us_595666f0e4b0da2c73230b40              |Mom Hilariously Nails What Getting Ready Looks Like For Mothers               |PARENTS      |”Maybe she’s born with it ... Maybe she’s a tired mom who doesn’t have time for this.”                                  |0.25               |\n",
      "|https://www.huffingtonpost.com/entry/how-to-make-the-perfect-c_us_5b9dc24ee4b03a1dcc8c8503                                                    |How to Make the Perfect Chocolate Chip Cookie                                 |FOOD & DRINK |A cookie with the perfect combination of fat, flavor, and comfort. Who needs detox?                                     |0.21739130434782608|\n",
      "|https://www.huffingtonpost.com/entry/andrew-garfield-lip-syncs-whitney-houston-in-epic-drag-show-act_us_592ff5d1e4b0540ffc84b79b              |Andrew Garfield Lip-Syncs Whitney Houston In Epic Drag Show Act               |ENTERTAINMENT|With a back flip and everything.                                                                                        |0.17647058823529413|\n",
      "|https://www.huffingtonpost.com/entry/linked-by-their-choice-to-become-single-mothers-two-women-share-their-stories_us_59b91b63e4b0edff9717de69|Linked By Their Choice To Become Single Mothers, Two Women Share Their Stories|PARENTS      |The stories of an Idaho mom who gave birth and a California mom who adopted.                                            |0.17391304347826086|\n",
      "|https://www.huffingtonpost.com/entry/carol-brady_us_5b9c5c92e4b03a1dcc7e15be                                                                  |What Carol Brady Is Really Saying (INFOGRAPHIC)                               |PARENTING    |Here's the story of a lovely lady -– who somehow kept six kids, a husband, a housekeeper and a dog in perfect order with|0.16666666666666666|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#VALIDATION USING MANUAL JACCARD SIMILARITY - FORMULA \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, MinHashLSH\n",
    "from pyspark.sql.functions import col, array_intersect, array_union, size\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MinhashLSH_ManualJaccard\").getOrCreate()\n",
    "data_path = 'shared/data/Huffpost.json'\n",
    "df = spark.read.json(data_path)\n",
    "base_description = \"Kitten Born With Twisted Arms And Legs Finds A Mom Who Knows She’s Perfect\"\n",
    "tokenizer = Tokenizer(inputCol=\"short_description\", outputCol=\"words\")\n",
    "words_df = tokenizer.transform(df)\n",
    "base_df = spark.createDataFrame([(base_description,)], [\"short_description\"])\n",
    "base_words_df = tokenizer.transform(base_df)\n",
    "base_words = base_words_df.select(\"words\").first()[0]  # Collect words as a list\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=10000)\n",
    "featurized_df = hashingTF.transform(words_df)\n",
    "minhash = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n",
    "model = minhash.fit(featurized_df)\n",
    "transformed_df = model.transform(featurized_df)\n",
    "base_features_df = hashingTF.transform(base_words_df)\n",
    "base_features = base_features_df.select(\"features\").first()[0]  # Get the feature vector\n",
    "similar_items_minhash = model.approxNearestNeighbors(transformed_df, base_features, numNearestNeighbors=5)\n",
    "\n",
    "# VALIDATION\n",
    "base_words_broadcast = spark.sparkContext.broadcast(set(base_words))\n",
    "\n",
    "# JACCARD SIMILARITY FORMULA\n",
    "def jaccard_similarity(words):\n",
    "    words_set = set(words)\n",
    "    intersection = words_set.intersection(base_words_broadcast.value)\n",
    "    union = words_set.union(base_words_broadcast.value)\n",
    "    return float(len(intersection)) / float(len(union)) if len(union) != 0 else 0.0\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "jaccard_udf = udf(jaccard_similarity, DoubleType())\n",
    "jaccard_df = words_df.withColumn(\"jaccard_similarity\", jaccard_udf(col(\"words\")))\n",
    "top_jaccard_items = jaccard_df.orderBy(col(\"jaccard_similarity\").desc()).limit(5)\n",
    "print(\"Top 5 items using MinHashLSH:\")\n",
    "similar_items_minhash.select(\"link\", \"headline\", \"category\", \"short_description\").show(truncate=False)\n",
    "\n",
    "print(\"Top 5 items using Manual Jaccard Similarity:\")\n",
    "top_jaccard_items.select(\"link\", \"headline\", \"category\", \"short_description\", \"jaccard_similarity\").show(truncate=False)\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata-spark]",
   "language": "python",
   "name": "conda-env-bigdata-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
